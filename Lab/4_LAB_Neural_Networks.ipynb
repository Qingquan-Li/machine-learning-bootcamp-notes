{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "- Design different NN structures for California Housing Data. Use different   \n",
        "  - number of hidden layers\n",
        "  - number of neurons for each layer\n",
        "  - activation function for each layer\n",
        "\n",
        "- Do not forget to split the whole data into training and test.\n",
        "- Use Keras\n"
      ],
      "metadata": {
        "id": "BzLIsHTyHRpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution"
      ],
      "metadata": {
        "id": "2SH2fdy3HY4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lOAm92PiHKxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3da61d-1861-49a1-dab1-f292e4ab1c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 64)                576       \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 128)               1152      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,473\n",
            "Trainable params: 9,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 256)               2304      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,521\n",
            "Trainable params: 43,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "465/465 [==============================] - 4s 6ms/step - loss: 1.2129 - val_loss: 0.5555\n",
            "Epoch 2/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.4736 - val_loss: 0.4523\n",
            "Epoch 3/10\n",
            "465/465 [==============================] - 3s 7ms/step - loss: 0.4169 - val_loss: 0.4358\n",
            "Epoch 4/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.3927 - val_loss: 0.4213\n",
            "Epoch 5/10\n",
            "465/465 [==============================] - 3s 5ms/step - loss: 0.3796 - val_loss: 0.4158\n",
            "Epoch 6/10\n",
            "465/465 [==============================] - 2s 3ms/step - loss: 0.3732 - val_loss: 0.4040\n",
            "Epoch 7/10\n",
            "465/465 [==============================] - 2s 3ms/step - loss: 0.3624 - val_loss: 0.4012\n",
            "Epoch 8/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.3552 - val_loss: 0.3941\n",
            "Epoch 9/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.3580 - val_loss: 0.3882\n",
            "Epoch 10/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.3476 - val_loss: 0.3865\n",
            "Epoch 1/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.7366 - val_loss: 0.5515\n",
            "Epoch 2/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.4707 - val_loss: 0.5287\n",
            "Epoch 3/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.4516 - val_loss: 0.4887\n",
            "Epoch 4/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.4313 - val_loss: 0.4813\n",
            "Epoch 5/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3997 - val_loss: 0.4452\n",
            "Epoch 6/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3768 - val_loss: 0.4094\n",
            "Epoch 7/10\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.3584 - val_loss: 0.3979\n",
            "Epoch 8/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3464 - val_loss: 0.3762\n",
            "Epoch 9/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3798\n",
            "Epoch 10/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3256 - val_loss: 0.3696\n",
            "Epoch 1/10\n",
            "465/465 [==============================] - 2s 3ms/step - loss: 1.1629 - val_loss: 0.5808\n",
            "Epoch 2/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.4547 - val_loss: 0.4583\n",
            "Epoch 3/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.4057 - val_loss: 0.4407\n",
            "Epoch 4/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.4441\n",
            "Epoch 5/10\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.3871 - val_loss: 0.4651\n",
            "Epoch 6/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.4285\n",
            "Epoch 7/10\n",
            "465/465 [==============================] - 2s 3ms/step - loss: 0.3776 - val_loss: 0.4250\n",
            "Epoch 8/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.3738 - val_loss: 0.4208\n",
            "Epoch 9/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3715 - val_loss: 0.4326\n",
            "Epoch 10/10\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.3634 - val_loss: 0.4327\n",
            "129/129 [==============================] - 0s 1ms/step - loss: 0.3588\n",
            "129/129 [==============================] - 0s 1ms/step - loss: 0.3373\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.3947\n",
            "Model 1 Loss: 0.3587570786476135\n",
            "Model 2 Loss: 0.33733999729156494\n",
            "Model 3 Loss: 0.3947250545024872\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the data\n",
        "dataset_cal = fetch_california_housing()\n",
        "X = dataset_cal.data\n",
        "y = dataset_cal.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Design different neural network architectures\n",
        "\n",
        "# Model 1: 1 hidden layer with 64 neurons, relu activation\n",
        "model1 = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(1)\n",
        "])\n",
        "model1.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "model1.summary()\n",
        "\n",
        "# Model 2: 2 hidden layers with 128 and 64 neurons, tanh activation\n",
        "model2 = Sequential([\n",
        "    Dense(128, activation='tanh', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='tanh'),\n",
        "    Dense(1)\n",
        "])\n",
        "model2.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "model2.summary()\n",
        "\n",
        "# Model 3: 3 hidden layers with 256, 128, and 64 neurons, sigmoid activation\n",
        "model3 = Sequential([\n",
        "    Dense(256, activation='sigmoid', input_shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='sigmoid'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(1)\n",
        "])\n",
        "model3.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "model3.summary()\n",
        "\n",
        "# Train the models\n",
        "model1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model3.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the models on the test set\n",
        "loss1 = model1.evaluate(X_test, y_test)\n",
        "loss2 = model2.evaluate(X_test, y_test)\n",
        "loss3 = model3.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Model 1 Loss: {loss1}\")\n",
        "print(f\"Model 2 Loss: {loss2}\")\n",
        "print(f\"Model 3 Loss: {loss3}\")\n"
      ]
    }
  ]
}